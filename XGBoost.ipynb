{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning with XGBoost\n",
    "\n",
    "\n",
    "Â©2022 MetaSnake\n",
    "\n",
    "`@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.5 please\n",
    "!pip install -U yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "We will also use SHAP, xgbfir, openpyxl, hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for colab\n",
    "!pip install -U xgboost xlrd yellowbrick dtreeviz feature_engine xgbfir shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import dtreeviz\n",
    "except ImportError:\n",
    "    print(\"No dtreeviz\")\n",
    "from feature_engine import encoding, imputation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import base, compose, datasets, ensemble, \\\n",
    "    metrics, model_selection, pipeline, preprocessing, tree\n",
    "import xgboost as xgb\n",
    "import yellowbrick.model_selection as ms\n",
    "from yellowbrick import classifier\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "I'll be demoing with Titanic ðŸ¤”ðŸ˜‰\n",
    "Your lab will be with Kaggle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web\n",
    "titanic_df = pd.read_excel('https://github.com/mattharrison/datasets/raw/master/data/titanic3.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local\n",
    "titanic_df = pd.read_csv('data/titanic3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X = titanic_df.drop(columns='survived')\n",
    "titanic_y = titanic_df.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweakTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    def __init__(self, ycol=None):\n",
    "        self.ycol = ycol\n",
    "        self.y_val = None\n",
    "        \n",
    "    def transform(self, X):\n",
    "        df = (X\n",
    "             .drop(columns=['name', 'ticket', 'home.dest', 'boat', 'body', 'cabin'])\n",
    "            )\n",
    "        if self.ycol in df:\n",
    "            self.y_val = df[self.ycol]\n",
    "        return df\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "\n",
    "titanic_pl = pipeline.Pipeline([('tweak', TweakTransformer()),\n",
    "    ('cat_impute', imputation.CategoricalImputer(imputation_method='missing',\n",
    "            variables=['sex', 'embarked'])),\n",
    "    ('cat', encoding.OneHotEncoder(top_categories=5, drop_last=True, \n",
    "            variables=['sex', 'embarked'])),\n",
    "    ('num_impute', imputation.MeanMedianImputer(imputation_method='median',\n",
    "                                               variables=['age', 'fare']))\n",
    "                       ])\n",
    "\n",
    "titanic_X_train, titanic_X_test, titanic_y_train, titanic_y_test = model_selection.train_test_split(\n",
    "    titanic_X, titanic_y, random_state=42, stratify=titanic_y)\n",
    "titanic_pl.fit_transform(titanic_X_train, titanic_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web\n",
    "url = 'https://github.com/mattharrison/datasets/raw/master/data/kaggle-survey-2018.zip'\n",
    "fin = urllib.request.urlopen(url)\n",
    "data = fin.read()\n",
    "os.makedirs('data')\n",
    "with open('data/kaggle-survey-2018.zip', mode='wb') as fout:\n",
    "    fout.write(data)\n",
    "with zipfile.ZipFile('data/kaggle-survey-2018.zip') as z:\n",
    "    print(z.namelist())\n",
    "    kag = pd.read_csv(z.open('multipleChoiceResponses.csv'))\n",
    "    kag_questions = kag.iloc[0]\n",
    "    raw = kag.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "with zipfile.ZipFile('data/kaggle-survey-2018.zip') as z:\n",
    "    print(z.namelist())\n",
    "    kag = pd.read_csv(z.open('multipleChoiceResponses.csv'))\n",
    "    kag_questions = kag.iloc[0]\n",
    "    raw = kag.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn(ser, n=5, default='other'):\n",
    "    counts = ser.value_counts()\n",
    "    return ser.where(ser.isin(counts.index[:n]), default)\n",
    "\n",
    "def tweak_kag(df):\n",
    "    return (df\n",
    "        #.query('Q3.isin([\"United States of America\", \"China\", \"India\"]) '\\\n",
    "        #       'and Q6.isin([\"Data Scientist\", \"Software Engineer\"])')\n",
    "        .loc[df.Q3.isin([\"United States of America\", \"China\", \"India\"]) &\n",
    "             df.Q6.isin([\"Data Scientist\", \"Software Engineer\"])]\n",
    "        .pipe(lambda df_:\n",
    "            df_.assign(**(df_.Q1.pipe(pd.get_dummies, drop_first=True, prefix='gender')),\n",
    "                       age=df_.Q2.str.slice(0,2).astype(int),\n",
    "                       **(df_.Q3.pipe(pd.get_dummies, drop_first=True, prefix='country')),\n",
    "                       education=df_.Q4.replace({'Masterâ€™s degree': 18,\n",
    "                         'Bachelorâ€™s degree': 16,\n",
    "                         'Doctoral degree': 20,\n",
    "                         'Some college/university study without earning a bachelorâ€™s degree': 13,\n",
    "                         'Professional degree': 19,\n",
    "                         'I prefer not to answer': None,\n",
    "                         'No formal education past high school': 12}),\n",
    "                       **(df_.Q5\n",
    "                              .pipe(topn, n=3)\n",
    "                              .replace({\n",
    "                        'Computer science (software engineering, etc.)': 'cs',\n",
    "                        'Engineering (non-computer focused)': 'eng',\n",
    "                        'Mathematics or statistics': 'stat'})\n",
    "                              .pipe(pd.get_dummies, drop_first=True, prefix='major')),\n",
    "                       title=df_.Q6,\n",
    "                       years_exp=(df_.Q8.str.replace('+','', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .astype(float)),\n",
    "                       compensation=(df_.Q9.str.replace('+','', regex=False)\n",
    "                           .str.replace(',','', regex=False)\n",
    "                           .str.replace('500000', '500', regex=False)\n",
    "                           .str.replace('I do not wish to disclose my approximate yearly compensation', '0', regex=False)\n",
    "                           .str.split('-', expand=True)\n",
    "                           .iloc[:,0]\n",
    "                           .fillna(0)\n",
    "                           .astype(int)\n",
    "                           .mul(1_000)\n",
    "                                    ),\n",
    "                       python=df_.Q16_Part_1.fillna(0).replace('Python', 1),\n",
    "                       r=df_.Q16_Part_2.fillna(0).replace('R', 1),\n",
    "                       sql=df_.Q16_Part_3.fillna(0).replace('SQL', 1)\n",
    "               )#assign\n",
    "              \n",
    "        )#pipe\n",
    "        .rename(columns=lambda col:col.replace(' ', '_'))\n",
    "        .loc[:, 'gender_Male':]   \n",
    "        .dropna()\n",
    "       )\n",
    "kag = tweak_kag(raw)\n",
    "kag_X = kag.drop(columns='title')\n",
    "kag_y = (kag.title == 'Data Scientist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kag_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stumps, Trees, and Forests\n",
    "\n",
    "Decision trees use a greedy algorithm to split on a feature (column) that results in the most \"pure\" split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict deatch and get 62% accuracy\n",
    "500/1309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "stump = tree.DecisionTreeClassifier(max_depth=1)\n",
    "X_train = titanic_pl.fit_transform(titanic_X_train)\n",
    "X_test = titanic_pl.transform(titanic_X_test)\n",
    "stump.fit(X_train, titanic_y_train)\n",
    "stump.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - died, 1 - survived\n",
    "stump.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in X_train.columns)\n",
    "_ = tree.plot_tree(stump, feature_names=features, filled=True, class_names=['Died', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfit\n",
    "A stump is too simple. It has too much *bias*.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "* Add more features\n",
    "* Use a more complex model\n",
    "\n",
    "For a tree we can let it grow deeper which should do both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "A model is too complicated. It has too much *variance*.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "* Simplify or constrain (*regularize*)\n",
    "* Add more samples\n",
    "\n",
    "For a tree we can prune back the growth so that the leaf nodes are overly specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hi_variance = tree.DecisionTreeClassifier(max_depth=None)\n",
    "X_train = titanic_pl.fit_transform(titanic_X_train)\n",
    "hi_variance.fit(X_train, titanic_y_train)\n",
    "hi_variance.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in X_train.columns)\n",
    "_=tree.plot_tree(hi_variance, feature_names=features, filled=True, class_names=['Died', 'Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit view to first 2\n",
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "features = list(c for c in X_train.columns)\n",
    "_ = tree.plot_tree(hi_variance, feature_names=features, filled=True, \n",
    "                 class_names=['Died', 'Survived'],\n",
    "                max_depth=2, ax=ax, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tree Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* 'max_depth=None' - Tree depth\n",
    "* 'max_features=None' - Amount of features to examine for split\n",
    "* 'max_leaf_nodes=None' - Number of leafs\n",
    "* 'min_impurity_decrease=0' - Split when *impurity* is >= this value. (*Impurity* : 0 - 100% accurate, .3 - 70%. Going from 70% to 100% accurate is a decrease of .3) \n",
    "* 'min_samples_leaf=1', - Minimum samples at each leaf.\n",
    "* 'min_samples_split=2' - Minimum samples required to split a node.\n",
    "* 'min_weight_fraction_leaf=0' - The fraction fo the total weights required to be a leaf.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(stump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Uses *bagging* to ensemble many trees in an attempt to lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "rf = ensemble.RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, titanic_y_train)\n",
    "rf.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(c for c in X_train.columns)\n",
    "fig, ax = plt.subplots(figsize=(8,10))\n",
    "_=tree.plot_tree(rf.estimators_[0], feature_names=features, filled=True,\n",
    "                 class_names=['Died', 'Survived'],\n",
    "                max_depth=2, ax=ax, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* 'n_estimators=100' - Number of trees.\n",
    "* 'oob_score=False' - Can estimate score when training (by using rows that weren't randomly selected). No need to hold out data\n",
    "* 'warm_start=False' - Can add more trees w/o starting over\n",
    "\n",
    "From tree:\n",
    "\n",
    "* 'max_depth=None' - Tree depth (1 to Infinity (`None`))\n",
    "* 'max_features=\"sqrt\"' - Amount of features to examine for split (1 to number of features (int). Float of percent (0. to 1.0). \"log2\" log2(n_features) or \"sqrt\"  sqrt(n_features). (Default square root number of features.)\n",
    "* 'max_leaf_nodes=None' - Number of leafs. Default (`None`) is unlimited.\n",
    "* 'min_impurity_decrease=0' - Split when *impurity* is >= this value. (0.0 to 1.0) (*Impurity* : 0 - 100% accurate, .3 - 70%) \n",
    "* 'min_samples_leaf=1', - Minimum samples at each leaf. (1 to n_samples).\n",
    "* 'min_samples_split=2' - Minimum samples required to split a node. (1 to n_samples)\n",
    "* 'min_weight_fraction_leaf=0' - The fraction (0.0 to 1.0) of the total weights required to be a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Show score estimated during training\n",
    "rf_oob = ensemble.RandomForestClassifier(random_state=42, oob_score=True)\n",
    "rf_oob.fit(X_train, titanic_y_train)\n",
    "rf_oob.score(X_test, titanic_y_test), rf_oob.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that the .score result changes after raising n_estimators\n",
    "rf_ws = ensemble.RandomForestClassifier(random_state=42, oob_score=True, \n",
    "                                        warm_start=True, n_estimators=3)\n",
    "rf_ws.fit(X_train, titanic_y_train)\n",
    "print(rf_ws.score(X_test, titanic_y_test), rf_oob.oob_score_)\n",
    "\n",
    "# change parameter and call .fit again. Doesn't need to start over\n",
    "rf_ws.set_params(n_estimators=200)\n",
    "rf_ws.fit(X_train, titanic_y_train)\n",
    "rf_ws.score(X_test, titanic_y_test), rf_oob.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize how changing n_estimators affects score\n",
    "results = []\n",
    "rf_ws = ensemble.RandomForestClassifier(random_state=42, warm_start=True, n_estimators=1)\n",
    "rf_ws.fit(X_train, titanic_y_train)\n",
    "for i in range(2,100):\n",
    "    rf_ws.set_params(n_estimators=i)\n",
    "    rf_ws.fit(X_train, titanic_y_train)\n",
    "    #results.append(rf_ws.score(X_test, titanic_y_test))\n",
    "    results.append(metrics.f1_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "    #results.append(metrics.precision_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "    #results.append(metrics.recall_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "    #results.append(metrics.roc_auc_score(titanic_y_test, rf_ws.predict(X_test)))\n",
    "pd.Series(results).plot(figsize=(8,4))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Exercise\n",
    "\n",
    "* Create a decision tree for the Kaggle Data predicting whether title is data scientist or software engineer\n",
    "* What is the score?\n",
    "* Create a random forest for the Kaggle Data predicting whether title is data scientist or software engineer\n",
    "* What is the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "Uses *boosting* to train a series of (weak) trees that try to correct the error of the previous output. (For classification this is mapped to a probability)\n",
    "\n",
    "Like golfing (you continue to putt or use a different club depending on first error). Decision tree would be a single tee off. Random forest would be averaging the tee offs. \n",
    "\n",
    "* Regularization\n",
    "* Parallel Processing\n",
    "* Missing Number Support\n",
    "* Category Support\n",
    "\n",
    "`'logloss'` is the default classification metric. (See https://xgboost.readthedocs.io/en/stable/parameter.html )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(eval_metric='logloss')\n",
    "xg.fit(X_train, titanic_y_train)\n",
    "xg.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.XGBClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's try w/ depth of 2 and 2 trees\n",
    "xg = xgb.XGBClassifier(max_depth=2, n_estimators=2)\n",
    "xg.fit(X_train, titanic_y_train)\n",
    "xg.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tree\n",
    "xgb.to_graphviz(xg, size='1,1', num_trees=0, fontsize='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second tree\n",
    "xgb.to_graphviz(xg, size='1,1', num_trees=1, fontsize='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go down the left path with\n",
    "# this data\n",
    "row = pd.Series({'pclass': 2.0,\n",
    " 'age': 28.0,\n",
    " 'sibsp': 8.0,\n",
    " 'parch': 2.0,\n",
    " 'fare': 69.55,\n",
    " 'sex_male': 0.0,\n",
    " 'sex_female': 1.0,\n",
    " #'sex_Missing': 0.0,\n",
    " 'embarked_S': 1.0,\n",
    " 'embarked_C': 0.0,\n",
    " 'embarked_Q': 0.0,\n",
    " #'embarked_Missing': 0.0\n",
    "                }).to_frame().T\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result for survival = .7027\n",
    "# > .5 ... so Survives!\n",
    "# this is [prob death, prob survival]\n",
    "xg.predict_proba(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# sum up leafs and throw into \n",
    "# Example: male, class 3\n",
    "# .49 + .37\n",
    "\n",
    "vals = np.linspace(-10, 10)\n",
    "def inv_logit(p):\n",
    "    return np.exp(p) / (1 + np.exp(p))\n",
    "\n",
    "print(inv_logit(.49+.37))\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(vals, inv_logit(vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Aside get trees from xgboost model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = xg.get_booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.trees_to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Exercise\n",
    "\n",
    "* Create an XGBoost tree for the Kaggle Data predicting whether title is data scientist or software engineer\n",
    "* What is the score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Benefit\n",
    "\n",
    "(Experimental) Support to handle missing values and catgeoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2_X_train, titanic2_X_test, titanic2_y_train, titanic2_y_test = model_selection.train_test_split(\n",
    "    titanic_df.drop(columns=['survived', 'boat', 'body']), titanic_df.survived, random_state=42, \n",
    "    stratify=titanic_df.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable_categorical is 1.5+ feature \n",
    "# Only supported with `gpu_hist`, `approx`, and `hist`.\n",
    "xg2 = xgb.XGBClassifier(enable_categorical=True, tree_method='hist')\n",
    "xg2.fit(titanic2_X_train, titanic2_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get rid of `object` types and convert them to categories\n",
    "titanic2_X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lots of columns have missing values!\n",
    "titanic2_X_train.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable_categorical is 1.5 feature\n",
    "xg2 = xgb.XGBClassifier(enable_categorical=True, tree_method='hist')\n",
    "xg2.fit(titanic2_X_train.assign(**titanic2_X_train.select_dtypes(object).astype('category')), titanic2_y_train)\n",
    "xg2.score(titanic2_X_test.assign(**titanic2_X_test.select_dtypes(object).astype('category')), titanic2_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(tree_method='hist')\n",
    "xg.fit(X_train, titanic_y_train)\n",
    "xg.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg2.get_booster().trees_to_dataframe().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tree (a little wide)\n",
    "xgb.to_graphviz(xg2, size='1,1', num_trees=0, fontsize='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's drop a few more columns that aren't really categorical\n",
    "titanic3_X_train, titanic3_X_test, titanic3_y_train, titanic3_y_test = model_selection.train_test_split(\n",
    "    titanic_df.drop(columns=['name', 'ticket', 'home.dest', 'boat', 'body', 'cabin', 'survived']), titanic_df.survived, random_state=42, \n",
    "    stratify=titanic_df.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable_categorical is 1.5 feature\n",
    "xg3 = xgb.XGBClassifier(enable_categorical=True, tree_method='hist')\n",
    "xg3.fit(titanic3_X_train.assign(**titanic3_X_train.select_dtypes(object).astype('category')), titanic2_y_train)\n",
    "xg3.score(titanic3_X_test.assign(**titanic3_X_test.select_dtypes(object).astype('category')), titanic2_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Parameters\n",
    "\n",
    "Attributes ending in underscore are calculated during the `.fit` call according to sklearn convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([att for att in dir(xg) if att.endswith('_') and not att.endswith('__')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Because you can keep \"putting\" you can keep track of how far away you are from the hole and stop when you are closest.\n",
    "\n",
    "If you set `early_stopping_rounds` and `eval_set`, you will use `.best_ntree_limit` when predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "# Go up to 100 but stop after you haven't improved for 20 hits\n",
    "# Min value at round 10 validation_1-logloss:0.40012\n",
    "# For v1.6+ move early_stopping_rounds to constructor\n",
    "\n",
    "xg = xgb.XGBClassifier(use_label_encoder=False)\n",
    "xg.fit(X_train, titanic_y_train,\n",
    "       eval_set=[(X_train, titanic_y_train),\n",
    "                 (X_test, titanic_y_test)],\n",
    "            early_stopping_rounds=20) \n",
    "xg.score(X_test, titanic_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the evaluation metrics\n",
    "# validation_0 is for training data\n",
    "# validation_1 is for testing data\n",
    "results = xg.evals_result()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the evaluation metrics\n",
    "results['validation_0']['logloss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing score is best at 11 trees\n",
    "results = xg.evals_result()\n",
    "ax = pd.DataFrame({'training': results['validation_0']['logloss'],\n",
    "              'testing': results['validation_1']['logloss'],\n",
    "             }).shift().plot(figsize=(5,4))\n",
    "ax.set_xlabel('ntrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see docs for ntree_limit on .predict\n",
    "xg.predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Hyperparameters\n",
    "\n",
    "*max_\\** parameters - Raise to make more complex (overfit|more variance), lower to simplify (underfit|more bias)\n",
    "\n",
    "*min_\\** parameters - Lower to make more complex (overfit|more variance), raise to simplify (underfit|more bias)\n",
    "\n",
    "* Boosting\n",
    "\n",
    "  * ``n_estimators=100`` - number of trees (or boosting rounds). Larger is more complex. Default 100. Use ``early_stopping_rounds`` with ``.fit`` to prevent overfitting.\n",
    "\n",
    "  * ``learning_rate=.3`` (called ``eta`` too) - after each boosting step, shrink feature weights. Larger is more conservative. Can be used with n_estimators to adjust time for convergence [0,1], default .3\n",
    "\n",
    "  * ``gamma=0`` / ``min_split_loss`` - L0 regularization. Global regularization. Minimum loss required for split. Larger is more conservative. [0, âˆž], default 0 - No regularization.\n",
    "\n",
    "\n",
    "* Regularization\n",
    "\n",
    "  * ``reg_lambda=1`` - L2 regularization (Root of squared weights). Increase to be more conservative. Default 1\n",
    "  * ``reg_alpha=0`` - L1 regularization (Mean of weights). Increase to be more conservative. Default 0\n",
    "\n",
    "* Sampling - Use different rows\n",
    "\n",
    "  * ``subsample=1`` - Use % of samples (this is rows!) for next boosting round. Lower to more conservative. [0, 1], default 1. (When not equal to 1.0, model does *stochastic gradient descent*, ie. there is some randomness in the model.)\n",
    "\n",
    "\n",
    "New tree (sampling) parameters - Use different columns (not rows!):\n",
    "\n",
    "  * ``colsample_bytree=1`` - Fraction of columns for each boosting round.\n",
    "  \n",
    "  * ``colsample_bylevel=1`` - Fraction of columns for each depth level.\n",
    "  \n",
    "  * ``colsample_bynode=1`` - Fraction of columns for each node.\n",
    "  \n",
    "\n",
    "From tree:\n",
    "\n",
    "  * ``max_depth=6`` - depth of tree. Larger is more complex (more likely to overfit). How many feature interactions you can have. Each level doubles time. [0, âˆž], default 6\n",
    "  * ``min_child_weight=1`` - Stop splitting after certain amount of purity. Larger will be more conservative.\n",
    "\n",
    "\n",
    "Imbalanced data:\n",
    "\n",
    "* ``scale_pos_weight=1`` -  ratio negative/positive. Default 1\n",
    "* Use ``'auc'`` or ``'aucpr'`` for ``eval_metric`` metric (rather than classification default ``'logless'``)\n",
    "* ``max_delta_step=0`` - try values from 1-10. Default 0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(),\n",
    "                    X_train, titanic_y_train,\n",
    "                    param_name='gamma', param_range=[0, .5, 1,2,5,10, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare max_depth for a Decision Tree first\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(tree.DecisionTreeClassifier(), X_train, titanic_y_train,\n",
    "                    param_name='max_depth', param_range=[1,2,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then for xgboost\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss'), X_train, titanic_y_train,\n",
    "                    param_name='max_depth', param_range=[1,2,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this depends on n_estimators\n",
    "# should really use early stopping but yellowbrick doesn't support this ðŸ˜¢\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(), X_train, titanic_y_train,\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# this takes a while to run (about 2 minutes)\n",
    "# can set scoring in GridSearchCV to \n",
    "# recall, precision, f1, accuracy\n",
    "params = {'reg_lambda': [0],  # No effect\n",
    "          'learning_rate': [.1, .3], # makes each boost more conservative (0 - no shrinkage) \n",
    "          'subsample': [.7, 1],\n",
    "          #'gamma': [0, 1],\n",
    "          'max_depth': [2, 3],\n",
    "          'random_state': [42],\n",
    "          'n_jobs': [-1],\n",
    "          'n_estimators': [200]}\n",
    "xgb2 = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "cv = (model_selection.GridSearchCV(xgb2, params, cv=3, n_jobs=-1)\n",
    "    .fit(X_train, titanic_y_train,\n",
    "         eval_set=[(X_test, titanic_y_test)],\n",
    "         early_stopping_rounds=5) \n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'learning_rate': 0.1,\n",
    " 'max_depth': 3,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs default\n",
    "xgb_def = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_def.fit(X_train, titanic_y_train)\n",
    "#xgb_grid = xgb.XGBClassifier(**cv.best_params_, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_grid = xgb.XGBClassifier(**params, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_grid.fit(X_train, titanic_y_train)\n",
    "xgb_def.score(X_test, titanic_y_test), xgb_grid.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Exercise\n",
    "\n",
    "* Use grid search to evaluate hyperparameters for your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Tuning with Hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our basic grid search example\n",
    "params = {'learning_rate': 0.1,\n",
    " 'max_depth': 3,\n",
    " 'n_estimators': 200,\n",
    " 'n_jobs': -1,\n",
    " 'random_state': 42,\n",
    " 'reg_lambda': 0,\n",
    " 'subsample': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "X_train = titanic_pl.fit_transform(titanic_X_train)\n",
    "X_test = titanic_pl.transform(titanic_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs default\n",
    "xgb_def = xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_def.fit(X_train, titanic_y_train)\n",
    "xgb_grid = xgb.XGBClassifier(**params, eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_grid.fit(X_train, titanic_y_train)\n",
    "xgb_def.score(X_test, titanic_y_test), xgb_grid.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.metrics import accuracy_score  \n",
    "#https://bradleyboehmke.github.io/xgboost_databricks_tuning/index.html#slide21\n",
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -7, 0),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 12, 1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -2, 3),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'gamma': hp.loguniform('gamma', -10, 10),\n",
    "    'reg_alpha': hp.loguniform('alpha', -10, 10),\n",
    "    'reg_lambda': hp.loguniform('lambda', -10, 10),\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'seed': 123,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(space):    \n",
    "    model = xgb.XGBClassifier(max_depth = int(space['max_depth']), \n",
    "                gamma = space['gamma'],                                         \n",
    "                reg_alpha = int(space['reg_alpha']),\n",
    "                min_child_weight=space['min_child_weight'],                                 \n",
    "                colsample_bytree=space['colsample_bytree'])\n",
    "    evaluation = [(X_train, titanic_y_train),\n",
    "            (X_test, titanic_y_test)]\n",
    "    model.fit(X_train, titanic_y_train,\n",
    "                 eval_set=evaluation, eval_metric=\"rmse\",            \n",
    "                 early_stopping_rounds=10,verbose=False)    \n",
    "         \n",
    "    pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(titanic_y_test, pred>0.5)    \n",
    "    print (\"SCORE:\", accuracy)    \n",
    "    #change the metric if you like    \n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=hyperparameter_tuning,            \n",
    "    space=space,           \n",
    "    algo=tpe.suggest,            \n",
    "    max_evals=1000,            \n",
    "    trials=trials,\n",
    "    #timeout=60*5 # 5 minutes\n",
    "           )\n",
    "print (best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best # new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train),\n",
    "            (X_test, titanic_y_test)]\n",
    "xgb_hyp.fit(X_train, titanic_y_train, early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp.score(X_test, titanic_y_test)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vs default and grid\n",
    "xgb_def.score(X_test, titanic_y_test), xgb_grid.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Now that we've tuned our model, let's look at how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train),\n",
    "            (X_test, titanic_y_test)]\n",
    "xgb_hyp.fit(X_train, titanic_y_train, early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(titanic_y_test, xgb_hyp.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default model\n",
    "metrics.accuracy_score(titanic_y_test, xg.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.confusion_matrix(xgb_hyp, X_train, titanic_y_train,\n",
    "                            X_test, titanic_y_test,\n",
    "                            classes=['Died', 'Survived']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "metrics.RocCurveDisplay.from_estimator(xgb_hyp,\n",
    "                       X_test, titanic_y_test,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.precision_recall_curve(xgb_hyp, X_train, titanic_y_train,\n",
    "                   X_test, titanic_y_test,\n",
    "                   classes=['Died', 'Survived'],\n",
    "                   micro=False, macro=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "classifier.classification_report(xgb_hyp, X_train, titanic_y_train,\n",
    "                   X_test, titanic_y_test,\n",
    "                   classes=['Died', 'Survived'],\n",
    "                   micro=False, macro=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Exercise\n",
    "* Create a Confusion Matrix for your model\n",
    "* Create an ROCAUC curve \n",
    "* Create a PrecisionRecall curve\n",
    "* How is the model performing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training For Different Metrics\n",
    "\n",
    "We tuned our model. But we tuned it against accuracy. What if we want to optimize for recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy tuning\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), \n",
    "                    X_train, titanic_y_train,\n",
    "    #                param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall tuning\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), \n",
    "                    X_train, titanic_y_train,\n",
    "                    scoring='recall',\n",
    "                    #param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ms.validation_curve(xgb.XGBClassifier(eval_metric='logloss', use_label_encoder=False), \n",
    "                    X_train, titanic_y_train,\n",
    "                    scoring='f1',\n",
    "                    #param_name='max_depth', param_range=[1,2,5,10]\n",
    "                    param_name='learning_rate', param_range=[0.001, .01, .1, .2, .5, .9, 1]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Trees are great when they overfit... They can explain what they overfit\n",
    "# (You can use these for \"surrogate models\")\n",
    "hi_variance = tree.DecisionTreeClassifier(max_depth=None)\n",
    "hi_variance.fit(X_train, titanic_y_train)\n",
    "hi_variance.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance shows the magnitude (not direction) of impact\n",
    "(pd.Series(hi_variance.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what happens\n",
    "dt3 = tree.DecisionTreeClassifier(max_depth=3)\n",
    "dt3.fit(X_train, titanic_y_train)\n",
    "\n",
    "dtreeviz.trees.dtreeviz(dt3, X_train, titanic_y_train, target_name='survivied',\n",
    "                       feature_names=X_train.columns, class_names=['died', 'survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost also supports feature importance\n",
    "xgb_def = xgb.XGBClassifier()\n",
    "xgb_def.fit(X_train, titanic_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None means 'gain'\n",
    "xgb_def.importance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gain is the default `importance_type`\n",
    "(pd.Series(xgb_def.feature_importances_, index=X_train.columns)\n",
    " .sort_values()\n",
    " .plot.barh()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify importance_type\n",
    "# * \"weight\" is the number of times a feature appears in a tree\n",
    "# * \"gain\" is the average gain of splits which use the feature\n",
    "# * \"cover\" is the average coverage of splits which use the feature\n",
    "xgb.plot_importance(xgb_def, importance_type='cover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the Model Exercise\n",
    "* What are the important features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgbfir (Feature Interactions Reshaped)\n",
    " *Gain*: Total gain of each feature or feature interaction\n",
    " \n",
    " *FScore*: Amount of possible splits taken on a feature or feature Interaction\n",
    " \n",
    " *wFScore*: Amount of possible splits taken on a feature or feature nteraction weighted by the probability of the splits to take place\n",
    " \n",
    " *Average wFScore*: wFScore divided by FScore\n",
    " \n",
    " *Average Gain*: Gain divided by FScore\n",
    " \n",
    " *Expected Gain*: Total gain of each feature or feature interaction weighted by the probability to gather the gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "xgbfir.saveXgbFI(xgb_def, feature_names=X_train.columns, OutputXlsxFile='fir.xlsx')\n",
    "pd.read_excel('fir.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('fir.xlsx', sheet_name='Interaction Depth 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interactions Exercise\n",
    "* What are the important features?\n",
    "* What are the second level interactions?\n",
    "* What are the third level interactions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP (SHapley Additive exPlantations)\n",
    "Should be *globally* consistent and accurate\n",
    "\n",
    " Shapley value (SHAP).\n",
    " \n",
    " From game theory, indicates how to distribute attribution of label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(xgb_def)\n",
    "vals = shap_ex.shap_values(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explain an individual\n",
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_def.predict(X_test.iloc[[0]])  # predicts death... why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label is also death\n",
    "titanic_y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values show direction of feature impact\n",
    "# for this individual\n",
    "pd.Series(vals[0], index=X_test.columns).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the base value. We sum up the scores.\n",
    "# > 0 Positive Case\n",
    "shap_ex.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < 0 therefore ... Death\n",
    "shap_ex.expected_value + vals[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib if having js issues\n",
    "# blue - Dead\n",
    "# red - Survival\n",
    "# colab needs initjs line\n",
    "shap.initjs()\n",
    "shap.force_plot(shap_ex.expected_value, \n",
    "               vals[0,:], X_test.iloc[0], #matplotlib=True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a feature\n",
    "shap.dependence_plot('age', shap_values=vals, features=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain another feature\n",
    "shap.dependence_plot('fare', vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain a feature with an interaction\n",
    "shap.dependence_plot('pclass', vals, X_test, interaction_index='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add jitter\n",
    "shap.dependence_plot('pclass', vals, X_test, interaction_index='age', x_jitter=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain global features\n",
    "shap.summary_plot(vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining the Model Exercise\n",
    "* Use SHAP to explore predictions for a sample\n",
    "* Use SHAP to expore a feature\n",
    "* Use SHAP to explore the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other XGBoost Features\n",
    "\n",
    "* Use the tree limit\n",
    "* Monotonic constraints - Force relationship of column to target to be monotonic\n",
    "* Interaction constraints - Limit interactions between features.\n",
    "* Can simulate decision trees (just one estimator) and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Limit\n",
    "Best practice is to use `early_stopping_rounds` with `eval_set` in call to `.fit`.\n",
    "\n",
    "If you set `n_estimators` in the constructor (`xgb.XGBClassifier`) and set those `early_stopping_rounds`, calls to `.predict` and `.score` will only use `.best_ntree_limit` trees, not necessaritly `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train),\n",
    "            (X_test, titanic_y_test)]\n",
    "xgb_hyp.fit(X_train, titanic_y_train, early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't even get to 2000 trees\n",
    "results = xgb_hyp.evals_result()\n",
    "ax = pd.DataFrame({'training': results['validation_0']['logloss'],\n",
    "              'testing': results['validation_1']['logloss'],\n",
    "             }).plot(figsize=(5,4))\n",
    "ax.set_xlabel('ntrees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp#.booster.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp.best_ntree_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses best_ntree_limit\n",
    "xgb_hyp.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(titanic_y_test,\n",
    "    xgb_hyp.predict(X_test, ntree_limit=xgb_hyp.best_ntree_limit)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(titanic_y_test,\n",
    "    xgb_hyp.predict(X_test, iteration_range=(0, 1588))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(titanic_y_test,\n",
    "    xgb_hyp.predict(X_test, iteration_range=(1000, 1000))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all of the trees is not necessarily better\n",
    "metrics.accuracy_score(titanic_y_test,\n",
    "    xgb_hyp.predict(X_test, iteration_range=(0, 1596))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monotonic Constraints\n",
    "If you want to remove u or w shaped behavior in features, force a monotonic constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# no constraints\n",
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "xgb_hyp = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                            use_label_encoder=False,\n",
    "                           n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train),\n",
    "            (X_test, titanic_y_test)]\n",
    "xgb_hyp.fit(X_train, titanic_y_train, early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp.score(X_test, titanic_y_test)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# add monotonic constraint to age\n",
    "# as age goes up survival goes down!\n",
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "# if age goes up survived (y) goes down (-1)\n",
    "constraints = [0 if col != 'age' else -1 \n",
    "              for col in X_train]\n",
    "cst = f'({\",\".join(map(str,constraints))})'\n",
    "# needs to be string '(1,0,0,-1,0,...)'\n",
    "xgb_hyp_age = xgb.XGBClassifier(**hyper_params, eval_metric='logloss', \n",
    "                                monotone_constraints=cst,\n",
    "                                use_label_encoder=False,\n",
    "                               n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train),\n",
    "            (X_test, titanic_y_test)]\n",
    "xgb_hyp_age.fit(X_train, titanic_y_train, early_stopping_rounds=10,\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp_age.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as second column (age) goes down, label goes up (survival)\n",
    "cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp_age.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our score with constraints is worse on accuracy\n",
    "xgb_hyp.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(xgb_hyp_age)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.dependence_plot('age', vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# no constraints\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "# make sure you initialize the js side\n",
    "shap_ex = shap.TreeExplainer(xgb_hyp)\n",
    "vals = shap_ex.shap_values(X_test)\n",
    "shap.dependence_plot('age', vals, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Constraints\n",
    "You can limit what columns interact with other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like sex_male has an interaction with pclass (and age)\n",
    "fig, ax = plt.subplots(figsize=(20,30), dpi=300)\n",
    "xgb.plot_tree(xgb_hyp, rankdir='LR', num_trees=0, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to specify index numbers of columns that can interact\n",
    "print(list(enumerate(X_train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it so pclass and sex_male|sex_female can't interact\n",
    "hyper_params ={'alpha': 0.0035215052171286097,\n",
    " 'colsample_bytree': 0.5362587947646607,\n",
    " 'gamma': 0.000561052978838129,\n",
    " 'lambda': 0.037563373793385646,\n",
    " 'learning_rate': 0.002795623033880572,\n",
    " 'max_depth': 3,\n",
    " 'min_child_weight': 0.5031984001674933,\n",
    " 'subsample': 0.9264026302532845}\n",
    "# Needs to be string with nested list of index values\n",
    "interaction_constraints = str([[idx for idx, col in enumerate(X_train.columns)\n",
    "                                if col not in {'pclass', 'sex_male', 'sex_female'}]])\n",
    "# column names don't work (even though docs say they do)\n",
    "#interaction_constraints = str([[col for idx, col in enumerate(X_train.columns)\n",
    "#                                if col not in {'pclass', 'sex_male', 'sex_female'}]]).replace(\"'\", '\"')\n",
    "\n",
    "xgb_hyp_age = xgb.XGBClassifier(**hyper_params\n",
    "                                interaction_constraints=interaction_constraints,\n",
    "                                early_stopping_rounds=10,\n",
    "                                use_label_encoder=False,\n",
    "                               n_estimators=2_000)\n",
    "evaluation = [(X_train, titanic_y_train),\n",
    "            (X_test, titanic_y_test)]\n",
    "xgb_hyp_age.fit(X_train, titanic_y_train\n",
    "           eval_set=evaluation)\n",
    "xgb_hyp_age.score(X_test, titanic_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index values for pclass, sex_* missing\n",
    "interaction_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(X_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_hyp_age.score(X_test, titanic_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,30), dpi=300)\n",
    "xgb.plot_tree(xgb_hyp_age, rankdir='LR', num_trees=1, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgbfir\n",
    "xgbfir.saveXgbFI(xgb_hyp_age, feature_names=X_train.columns, OutputXlsxFile='titanic_fir_interactions.xlsx')\n",
    "pd.read_excel('titanic_fir_interactions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like pclass has a non-monotonic effect (but it isn't interacting w/ other columns)\n",
    "pd.read_excel('titanic_fir_interactions.xlsx', sheet_name='Interaction Depth 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel('titanic_fir_interactions.xlsx', sheet_name='Interaction Depth 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "XGBoost is very powerful. Combining with other tools will take you a long way.\n",
    "\n",
    "Explore your data and your results.\n",
    "\n",
    "Lots of libraries. Some are better integrated.\n",
    "\n",
    "Suggestions:\n",
    "\n",
    "* Pandas skills come in useful for manipulating data\n",
    "* Make sure you discuss business value with stake holders\n",
    "\n",
    "\n",
    "Questions?\n",
    "\n",
    "\n",
    "Connect on LinkedIn or Twitter `@__mharrison__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randrange(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randrange(1,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
